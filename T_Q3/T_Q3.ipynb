{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b1d379",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Retreival_Augmented Generation (RAG) over PDF\n",
    "##### Goal: Build a RAG chatbot over a chosen PDF using open-source LLMs.\n",
    "---\n",
    "### Ingestion\n",
    "Parse PDF, split into chunks that make semantic sense\n",
    "\n",
    "To install all required libraries, run\n",
    "```bash\n",
    "pip install pymupdf4llm llama-index llama-index-embeddings-google-genai llama-index-llms-google-genai marker-pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7b21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.llms import MessageRole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30827d7",
   "metadata": {},
   "source": [
    "The following snippet asks for the pdf file to be uploaded.\n",
    "\n",
    "It also has 3 modes:\n",
    "1. **Quick**: Best for text-based pdfs (eg. `test_easy.pdf`)\n",
    "2. **Balanced**: Best for pdfs with tables, equations etc. (eg. `test_hard.pdf`)\n",
    "3. **Detailed**: Best for research papers, scanned pdfs and complex documents. (eg. `test_extreme.pdf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "291285e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Quick: Best for text-based pdfs, \n",
      "2. Balanced: Best for pdfs with tables, equations, images etc. \n",
      "3. Detailed: Best for research papers, scanned pdfs and complex documents.\n"
     ]
    }
   ],
   "source": [
    "file_name = input(\"Enter name of file to load (don't write file extension): \")\n",
    "print(\"1. Quick: Best for text-based pdfs, \\n2. Balanced: Best for pdfs with tables, equations, images etc. \\n3. Detailed: Best for research papers, scanned pdfs and complex documents.\")\n",
    "mode = input(\"Enter mode (1/2/3): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35bee0",
   "metadata": {},
   "source": [
    "For Quick mode, PyMuPDF4LLM is used. It uses heuristic methods to parse the pdf such as bounding boxes. \n",
    "\n",
    "For Balanced mode, marker-pdf is used which uses a combination of heuristic and OCR based methods to parse the pdf, giving more accurate output and support for Latex, tables and code snippets\n",
    "\n",
    "For Detailed mode, marker-pdf with the force_OCR flag is used, which is very accurate, but time consuming as well. This works well for newspaper-type pdfs, scientific papers where accuracy is paramount and scanned pdfs, where the text cannot be extracted normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b2abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"1\":\n",
    "    md_text = pymupdf4llm.to_markdown(f\"{file_name}.pdf\")\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "    pathlib.Path(f\"{file_name}/{file_name}.md\").write_bytes(md_text.encode())\n",
    "elif mode == \"2\":\n",
    "    os.system(f\"marker_single {file_name}.pdf --output_format markdown --output_dir ./ --disable_image_extraction\")\n",
    "    os.remove(f\"{file_name}/{file_name}_meta.json\")\n",
    "elif mode == \"3\":\n",
    "    os.system(f\"marker_single {file_name}.pdf --output_format markdown --output_dir ./ --force_ocr --disable_image_extraction --format_lines\")\n",
    "    os.remove(f\"{file_name}/{file_name}_meta.json\")\n",
    "else:\n",
    "    print(\"Invalid mode selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b73b8e",
   "metadata": {},
   "source": [
    "The previous snippet saves the output as markdown, since it captures the structure of document, equations, tables, code, links etc. \n",
    "\n",
    "Next, we load the document into the LlamaIndex, which is an orchestration framework for knowledge assistants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ceecd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=file_name)\n",
    "documents = reader.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103a0f5",
   "metadata": {},
   "source": [
    "The next step is chunking the document text into sensible semantic units. Since we use semantic chunking, a text embedding model must be used.\n",
    "\n",
    "We use Google's open source text embedding model due to its speed, reliability and SOTA technology. Enter your Google Cloud API key in the `api_key` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d87033",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    api_key=\"GOOGLE_API_KEY\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e76f7",
   "metadata": {},
   "source": [
    "There are largely 5 levels of chunking.\n",
    "1. Fixed size\n",
    "2. Recursive: split using certain characters like `\\n` or `.`\n",
    "3. Document based: split based on document structure like headings, code snippets, tables etc.\n",
    "4. Semantic: split based on semantic meaning of the text, using a text embedding model (**this is what we use**)\n",
    "5. Agentic: Uses an LLM to determine the best way to chunk the text based on context\n",
    "\n",
    "The first three methods are not effective enough ways to split text, as chunks aren't contextually similar. The last method is computationally expensive. Semantic chunking is the sweet spot, providing high performance and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c995f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147df5f0",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Next step in the process is to store the chunks in a vector database. Using top-k similarity search, we can retrieve the most reevant chunks from this database, based on the user query instead of parsing through the entire pdf over again.\n",
    "\n",
    "The last line in the snippet persists the index to disk. In Jupyter notebook, since memory of previous cells is stored, it is not necessary, in other cases, it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbfdb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "\n",
    "vector_index.storage_context.persist(persist_dir=\"<persist_dir>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e7d65",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "Now, we can load the index from disk. top-k value can be adjusted based on the size of the pdf. 10 is good enough for londer pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9094c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)\n",
    "\n",
    "\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47611bbf",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "Google's 2.0 flash is used as the response generation model due to its speed and reliability, and RPD limits (Requests per day). \n",
    "\n",
    "The query engine uses a similarity cutoff. This hyperparameter can be reduced if Empty response is generated.\n",
    "\n",
    "Enter your Google Cloud API key in the parameter `api_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0f97088",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=\"GOOGLE_API_KEY\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm=llm)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4caff",
   "metadata": {},
   "source": [
    "The following snippet asks for the user query and generates a response based on the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d595fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'quit' to exit, 'history' to see conversation history, or 'clear' to reset history.\n",
      "\n",
      "\n",
      "Assistant: Okay, I'm ready to answer questions about the provided documents. I understand the context includes information about downtrends, price movements, candle patterns, and criteria related to trend changes. I will do my best to provide accurate and helpful responses based on the information given.\n",
      "\n",
      "\n",
      "Conversation history cleared!\n",
      "\n",
      "\n",
      "Assistant: ```tool_code\n",
      "print(file_path)\n",
      "```\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Initialize chat memory for conversation history\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3000)\n",
    "\n",
    "# Create a history-aware chat engine\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"You are a helpful AI assistant that can answer questions about the provided PDF document. \"\n",
    "        \"Use the conversation history and the relevant document context to provide accurate and helpful responses.\\n\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"Instruction: Use the previous chat history and context to answer the question. \"\n",
    "        \"If the question refers to something mentioned earlier in the conversation, use that context appropriately.\"\n",
    "    ),\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.1)],\n",
    ")\n",
    "\n",
    "print(\"Type 'quit' to exit, 'history' to see conversation history, or 'clear' to reset history.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    elif user_input.lower() == 'clear':\n",
    "        memory.reset()\n",
    "        print(\"Conversation history cleared!\\n\")\n",
    "        continue\n",
    "    \n",
    "    response = chat_engine.chat(user_input)\n",
    "    print(f\"\\nAssistant: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b4791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
